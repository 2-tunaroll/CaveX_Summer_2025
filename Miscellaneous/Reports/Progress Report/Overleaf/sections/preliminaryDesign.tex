\section{Preliminary Design}
\label{sec:prelim-design}
As the 2023 iteration of the project primarily involves introducing autonomous functionality to the 2022 prototype, the design scope is largely limited to software improvements. Hardware changes only occur where they are necessary to facilitate the desired software implementation. As per this year's project objectives, the preliminary design work initially involves the selection of algorithms to support autonomous functionality. F-LOAM will serve as an accurate and computationally-efficient SLAM algorithm (OB1) in addition to underpinning the obstacle avoidance and path planning functionality described in OB4 and OB2, respectively. Further support of OB4 and OB2 will be provided through implementing a point cloud-based obstacle detection algorithm. In pursuit of autonomous navigation, a closed-loop terrain compensation algorithm will be implemented such that it integrates with the existing OpenSHC system to improve the prototype's traversal capabilities in a cave environment (OB4). Additionally, a real time system monitoring technology will be implemented to make the development and debugging pattern more efficient (OB5). These improvements are designed such that they facilitate the accomplishment of the project objectives as well as future iterations of the project.

\subsection{Mapping \& Localisation}
Endowing the prototype with the ability to perform real time mapping and localisation is primarily a software-related process due to the robot's existing LiDAR sensor permitting implementation of SLAM. However, while SLAM can be performed solely using LiDAR data, it typically leads to non-negligible cumulative errors (Guo et al. 2021). Therefore, additional data is required to negate or minimise such errors. One way to achieve this is to implement an IMU. Concurrent analysis of LiDAR and IMU data allows SLAM accuracy to be maintained over the course of a mission. In particular, this sensor fusion technique permits loop closure, an error-reducing strategy wherein the robot is detected to have returned to a position it was at earlier, to be performed more easily and accurately. This technique to minimises cumulative map drift (Karam, Lehtola \& Vosselman 2021).

The primary means of localisation is the implementation of a GPU-accelerated version of F-LOAM. F-LOAM determines the transform between two LiDAR scans via feature matching as it is more computationally-efficient than the classical Iterative Closest Point (ICP) method. Specifically, by extracting surfaces and edges, the number of points to consider in transform calculations is significantly reduced and therefore reduces run time (Wang et al. 2021). 

A nine degree of freedom (DOF) IMU continuously provides discrete linear acceleration, angular velocity, and magnetic field data in three axes each (Yadav \& Bleakley 2016). By taking the robot's starting position to be the origin of a fixed reference frame for both the LiDAR and IMU, the robot's pose can be continuously calculated for both in the same global coordinate system. Over time, these two representations of the robot's pose are liable to diverge due to unique measurement errors inherent to the sensors or algorithm (Guo et al. 2021; Yadav \& Bleakley 2016). Since the data from an IMU is typically non-linear and numerically differentiable, the Extended Kalman Filter (EKF) can be applied to minimise error in the integration of the data (Wang \& Li 2021). By maintaining a history of the robot's positions throughout a mission, the robot's current position can be continuously checked against the history for matches and the LiDAR data can be checked against it to correct error through loop closure. However, to ensure efficient matching the data must be structured appropriately. The k-dimensional tree (KD tree) is a data structure which effectively stores spatial data for performant search (Bereczky et al. 2016). This will permit efficient integration with the LiDAR localisation algorithm. 

As GPUs are designed for parallel computing, GPU acceleration can improve performance of parts of the F-LOAM algorithm where an identical arithmetic process is applied to or using a large number of points from the LiDAR point cloud (Xiao et al. 2022). Thus, the parts of the algorithm which are liable to realise performance gains through GPU acceleration are feature extraction, voxelisation, and cost factor calculation. As the robot's processing occurs on the NVIDIA Jetson platform, NVIDIA's GPU computing framework, Compute Unified Device Architecture (CUDA), will be used to introduce the aforementioned parallel processing (NVIDIA n.d.a).

\subsection{Navigation}
Providing an autonomous robotic system with navigational abilities involves overcoming three challenging tasks; implementing obstacle detection, obstacle avoidance, and path planning. Given a non-uniform and obstacle-laden terrain, obstacle detection underpins both avoidance and path planning. Obstacle avoidance and path planning have considerable overlap in purpose, but path planning aims to look further into the future and thus relies on short-term obstacle avoidance to do so.

\subsubsection{Obstacle Detection \& Avoidance}
Given that the robot has a LiDAR sensor but cannot have a camera due to its operating environment, obstacle detection will be conducted solely using the LiDAR data. The selected obstacle detection algorithm uses point cloud pre-processing, segmentation, IQR gating, and RANSAC-based plane fitting of LiDAR data to identify obstacles (Asvadi et al. 2016). Since the pre-processing involves voxelisation and point cloud registration, two tasks which have already occurred for SLAM, the processing step can be skipped. The segmentation step involves slicing, gating, plane fitting, and validation. Slicing, the process of dividing the point cloud into region-based segments to facilitate future analysis, requires the consideration of a large number of LiDAR points and thus can benefit from GPU acceleration. Gating, however, relies on finding medians, a process which is not suited to parallel computing. Similarly, RANSAC plane fitting is better suited to serial computation on the CPU as it is a light and low-throughput operation, meaning the additional time required to copy relevant data to the GPU is detrimental to performance (Seekhao et al. 2019). Therefore, in the interest of optimising performance, slicing will be conducted as part of the SLAM analysis before the data is transferred back to the Jetson's memory for further processing. For further optimisation, only points in a 150\textdegree \space horizontal FOV in front of the robot's direction of motion will be analysed for objects as static objects outside of this range are unlikely to impede the robot's operation. This can be realised by comparing a point's position in the robot's coordinate system to the robot's velocity vector. As the final part of ground segmentation, the validation process is conducted on the CPU as it is very computationally-light. Next, on-ground obstacles are segmented from the remaining points, the obstacle point set is voxelised using a voxel grid, and dynamic obstacles are segmented from static ones. Ground/on-ground separation is conducted by determining the distance of an arbitrary point in a slice from the ground plane and comparing it to a maximum allowable value. Dynamic-static obstacle segmentation can be achieved by determining if an obstacle is mapped to different voxels over time. This is accomplished by comparing the voxels in the broader map to the voxels in the obstacle voxel set. Finally, A two-dimensional (2D) counter and log-likelihood ratio are then applied to develop binary masks which assign voxels a label of stationary or moving. The 2D counter can be achieved by looping through and summing the relevant voxels on the CPU. Since the log-likelihood ratio depends on both the 2D counters, this will be calculated immediately after determination of the counters. The binary masks then compare the log-likelihood ratio to two arbitrary thresholds, one for static obstacles and one for dynamic obstacles, to label voxels accordingly (Asvadi et al. 2016). These thresholds, $T_d$ and $T_s$, will be determined by iterative experimentation.

\subsubsection{Pathfinding}
Based on the results of the object detection algorithm, pathfinding will occur using the artificial potential field method. To do this, the obstacles will be considered in the robot's coordinate system. Through the artificial potential field method, the ideal velocity vector can be continuously determined as the robot moves through its environment. Only the components of the velocity vector which are in the horizontal plane will be considered as the dynamic gait algorithm will automatically adjust the robot's orientation for changes in the vertical direction. After calculation, the velocity vector will be communicated to the control system through the \slash syropod\_remote\slash desired\_velocity ROS topic using the geometry\_msgs::Twist message type. Once the robot has avoided an obstacle, it will be removed from the obstacle list to reduce computational load.

\subsection{Dynamic Gait Optimisation}
The selected dynamic gait algorithm relies only on feedback from the Dynamixel motors and locomotion state information (Zangrandi, Arrigoni \& Braghin 2021). Since the motors are capable of providing feedback on the current draw, which can be converted to motor torque, and the locomotion state is stored by OpenSHC, no hardware changes are required to implement the algorithm (Robotis 2023a; Robotis 2023b; Tam et al. 2020). Currently, OpenSHC's walk and pose controllers generate the leg movement by calculating the desired joint positions (Tam et al. 2020). As the selected dynamic gait algorithm only deals with modifying the gait according to feedback, it will be integrated into the existing OpenSHC code base. The gait to be modified is the tripod gait as this provides the best performance and is, therefore, the ideal candidate for serving as a baseline. To introduce the new gait algorithm, the walk and pose controllers will be modified. The kinematic model built into OpenSHC renders that aspect of implementation simpler as the new functionality can piggy back off it, but the complexity of the code base will make it difficult to determine exactly what to modify. An iterative development and testing cycle will be required to successfully implement the algorithm. 


% Jetson Orin NX 8GB
\subsection{Computational Power}
The new functionality introduced in this iteration of the CaveX project is almost entirely software-based. Additionally, it relies on heavy processing of point cloud data. To accommodate such changes, the current NVIDIA Jetson Nano will be upgraded to an NVIDIA Jetson Orin NX 8GB. The data on the existing Nano will be transferred to the Orin NX prior to installation. To avoid a change in the robot's mass distribution, the Orin NX will be installed in the same position as the Nano. However, the Orin NX comes with a fan which the Nano does not have and, therefore, requires additional vertical space to fit and provide sufficient airflow for cooling. While data on the height of the Orin NX combined with its carrier board is not available, it is assumed that it matches the height of the Nano with its carrier board at 30.21mm (Seeed Studio n.d.; NVIDIA n.d.b). The additional fan adds up to 18mm and thus may necessitate an increase in the distance between the robot's middle and top layer of carbon fibre (DigiKey n.d.). In this case, the stand off screws which connect the three layers would need to be adjusted accordingly. 

% Real-time monitoring
\subsection{Development \& Debugging}
As mentioned in Section \ref{sec:dev-debug}, the process of deploying software onto the robot is currently inefficient. To improve this process, the team will use the Jenkins open-source automation server which is compatible with the Ubuntu operating system installed on the Jetson Nano. This software will allow for code on the CaveX GitHub repository to be continuously integrated and deployed on the robot code base whenever changes are made to the repository. The JenkinsFile which sets up the configuration of the Jenkins software can be manipulated to only trigger an automatic build process when changes to the main branch are made on the repository's main branch. This will prevent undesired builds and eliminate the possibility of code conflicts. Enabling an efficient development and debugging experience also requires real time interaction with the system. Due to bit rate and connectivity limitations of non-WiFi wireless communications technologies, the detailed data required for development and debugging cannot be transmitted in a connectionless cave environment. However, as most development and debugging occurs in a connected environment, DroneDeploy can be installed onto the system with Rocos for ROS integration (DroneDeploy n.d.a; DroneDeploy n.d.b). To achieve this, the WiFi module must first be connected to the Jetson carrier board. The DroneDeploy ROS agent can then be installed onto the robot and configured to stream data to a device. DroneDeploy with Rocos and the ROS agent provide features such as topic monitoring, topic exploration, bandwidth monitoring, point cloud rendering, data dashboards, and data streaming (DroneDeploy n.d.b; DroneDeploy 2022). Where unavailable features are required, they can be introduced using a custom web server and the Rocos SDK or the ROS Node.js package (DroneDeploy n.d.c; ROS Wiki 2017). 

\subsection{Summary}
Overall, with careful considerations to programmatic design, the various algorithms which are to endow the robot with autonomy can be implemented into the existing system. The SLAM algorithm, F-LOAM, is likely to be the most computationally expensive aspect of the system but it underpins most of the later autonomous decision making. Since the processing for SLAM partially overlaps with the processing for obstacle detection, the two processes can be merged for increased performance. Additionally, obstacle detection can be optimised by only considering points in the region ahead of the robot's direction of movement. The gait algorithm can be optimised by integrating it into OpenSHC's existing code base. Pathfinding and obstacle avoidance will be achieved using the artificial potential field method and build on the SLAM and obstacle detection data. To facilitate these systems, a new NVIDIA Jetson Orin NX will be installed along side a real time monitoring system for rapid development and debugging. These improvements also provide a strong basis for future iterations of the CaveX project.